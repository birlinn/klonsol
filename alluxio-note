Write–up of FCFRDM Spark 1.6 issues encountered and implemented workaround follows:
 
Issue
 
Spark 1.6 default JARs includes many Jackson JARs, specifically ‘jackson-databind-2.2.3.jar’. This creates a conflict with the ‘alluxio-enterprise-1.6.0’ JARs which make use of ‘jackson-databind-2.6.7’. The Alluxio client JARs are included in the Spark executor classpath from both the Spark 1.6 default classpath and from the Hadoop classpath.
 
Our Resolution
 
To make Spark 1.6 work with the Alluxio 1.6 client JAR, we removed all Jackson JARs from the client JAR and then replaced the old Alluxio JAR with the modified one. This fix was done for all Spark workers. Doing this resolved the versioning conflict.
 
Comments
 
The issue with our fix is that both Spark 1.6 and Spark 2.2 make use of the Hadoop classpath when creating the Spark classpath for the executors. Using the original Alluxio client JAR will cause a conflict with Spark 1.6. Using the modified JAR could cause issues with any services that do not have their own Jackson JARs. Replacing the Jackson JARs with an older version would cause version conflicts with Spark 2.2 for the same reason.
 
Potential Solutions
 
1. Run Spark 1.6 in a separate cluster.
2. Run Spark 1.6 and Spark 2.2 in the same cluster: shade the Alluxio client Jackson JARs and use the new shaded client for Spark 1.6 and Hadoop. NB: this could break any programs that rely on Hadoop Jackson JARs. Those programs would have to include the Jackson JARs on their own.
3. Run Spark 2.2 only: migrate application code to Spark 2.2.
 
Recommendation
 
The safest course of action would be to migrate Spark 1.6 code to Spark 2.2 (solution 3, above). It is probably advisable to carefully evaluate the migration path before initiating any work: some tricks to reduce the amount of work might be available.
 
